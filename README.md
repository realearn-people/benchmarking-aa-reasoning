## LLM Reasoning Benchmark for Abstract Argumentation

This project provides a comprehensive framework for evaluating and benchmarking the abstract argumentative reasoning capabilities of Large Language Models (LLMs). It uses Dung's Abstract Argumentation Frameworks (AFs) as a formal basis for testing.

The methodology is twofold:

1.  **Validity Testing**: The LLM's ability to compute correct argumentation extensions (Grounded, Complete, Preferred, Stable) is verified against the ground-truth solutions generated by the `py-arg` library.
2.  **Metamorphic Testing**: The logical consistency of the LLM's reasoning is probed by applying systematic transformations (e.g., Isomorphism, Modularity) to the argumentation frameworks and verifying that the relationships between the original and transformed outputs hold true.

## Project Structure

```
├── prompts/              # Contains system prompts for the LLMs
├── reports/              # Output directory for evaluation reports
├── src/
│   ├── main.py
│   ├── LLM_Interface.py
│   ├── LLM_tester.py
│   ├── LogicTester.py
│   ├── af_utils.py
│   ├── test_af_utils.py
│   └── ReportGenerator.py
├── requirements.txt      # Project dependencies
└── README.md             # This file
```

| File                     | Responsibility                                                                                                                              |
| :----------------------- | :------------------------------------------------------------------------------------------------------------------------------------------ |
| **`main.py`**            | The main entry point for the application. Configures and initiates the evaluation process.                                                  |
| **`LLM_Interface.py`**   | Contains the core classes that drive the evaluation: `LLMClient` (for API interaction), `VerificationSuite`, and `LLMTester`.               |
| **`LLM_tester.py`**      | Orchestrates the evaluation process for a single LLM.                                                                                       |
| **`LogicTester.py`**     | A specialized tester that uses ground-truth extensions instead of LLM outputs to verify that the verification logic itself is sound.        |
| **`ReportGenerator.py`** | A dedicated module containing the `ReportGenerator` class, which uses `pandas` to export the final results into a multi-sheet Excel file.   |
| **`af_utils.py`**        | A utility module containing functions to generate the six types of Argumentation Frameworks and apply the four metamorphic transformations. |
| **`test_af_utils.py`**   | A suite of unit tests to ensure the correctness and reliability of the functions in `af_utils.py`. Run using python's unittest module.      |

## Setup

1.  **Clone the repository:**

```bash
git clone https://github.com/realearn-people/benchmarking-aa-reasoning.git
cd benchmarking-aa-reasoning
```

### 2. Install Dependencies

Install all the required Python packages using the requirements.txt file.

```bash
pip install -r requirements.txt
```

3.  **Set up API Keys:**
    You need to provide API keys for the services you want to use. Create a `.env` file in the root directory or set them as environment variables:

    ```
    OPENAI_API_KEY="your-openai-api-key"
    GEMINI_API_KEY="your-google-gemini-api-key"
    ```

4.  **Using Local Models with Ollama:**
    If you want to test local models, make sure you have [Ollama](https://ollama.ai/) installed and running. You can pull models like Llama 3 by running:
    ```bash
    ollama pull llama3:8b
    ```

## How to Run

1.  **Configure the Evaluation**: Open `src/main.py`.

    - Choose the LLM(s) you want to test by uncommenting or adding the corresponding client initializations.
    - Add the configured models to the `list_models` list.
    - You can adjust the `af_generators_to_test` dictionary to select which AF structures to use.
    - You can change the `sizes_to_test` list to specify the number of arguments in the generated AFs.

2.  **Run the script:**
    ```bash
    python src/main.py
    ```

## How It Works

The evaluation process is driven by the `LLMTester` class. For each selected AF generator and size, it performs the following steps:

1.  **Base Test**: An initial AF is generated, and the LLM is queried for its extensions. The response is checked for fundamental correctness (e.g., conflict-freeness, admissibility).
2.  **Metamorphic Tests**: The base AF is transformed using one of the metamorphic relations, and the LLM is queried again with this new AF. The results from the original and the transformed AF are then compared to see if they satisfy the expected logical relationship.

For example, under the **isomorphism** test, the arguments in the AF are renamed. A logically consistent LLM should produce the same set of extensions, just with the new names.

## Results

The evaluation results are saved as an Excel file in the `reports/` directory. The filename is based on the model and configuration. The report contains a detailed breakdown of passes and fails for each test type and size, including the computed vs. expected extensions and any detected violations.

# Benchmarking Abstract Argumentative Reasoning of LLMs

This project provides a comprehensive framework for evaluating and benchmarking the abstract argumentative reasoning capabilities of Large Language Models (LLMs). It uses Dung's Abstract Argumentation Frameworks (AFs) as a formal basis for testing.

The methodology is twofold:
1.  **Validity Testing**: The LLM's ability to compute correct argumentation extensions (Grounded, Complete, Preferred, Stable) is verified against the ground-truth solutions generated by the `py-arg` library.
2.  **Metamorphic Testing**: The logical consistency of the LLM's reasoning is probed by applying systematic transformations (e.g., Isomorphism, Modularity) to the argumentation frameworks and verifying that the relationships between the original and transformed outputs hold true.

## Project Structure

The project is organized into several Python modules within the `src/` directory:

| File                  | Responsibility                                                                                                                        |
| :-------------------- | :------------------------------------------------------------------------------------------------------------------------------------ |
| **`main.py`** | The main entry point for the application. Configures and initiates the evaluation process.                                          |
| **`LLM_Interface.py`** | Contains the core classes that drive the evaluation: `LLMClient` (for API interaction), `VerificationSuite`, and `LLMTester`.        |
| **`af_utils.py`** | A utility module containing functions to generate the six types of Argumentation Frameworks and apply the four metamorphic transformations. |
| **`report_generator.py`** | A dedicated module containing the `ReportGenerator` class, which uses `pandas` to export the final results into a multi-sheet Excel file. |
| **`test_af_utils.py`** | A suite of unit tests to ensure the correctness and reliability of the functions in `af_utils.py`.                                  |
| **`LogicTester.py`** | A specialized tester that uses ground-truth extensions instead of LLM outputs to verify that the verification logic itself is sound.  |

## Setup and Installation

Follow these steps to set up the project environment.

### 1. Clone the Repository

Clone this repository to your local machine.

```bash
git clone <your-repository-url>
cd benchmarking-aa-reasoning
```

### 2. Install Dependencies
Install all the required Python packages using the requirements.txt file.

```bash
pip install -r requirements.txt
```

### 3. Configure Environment Variables
The framework requires API keys to interact with LLM services.
1. Create a .env file in the root directory of the project (benchmarking-aa-reasoning/.env).
2. Add your API keys to the .env file. The file should look like this:
```bash
OPENAI_API_KEY="sk-..."
GEMINI_API_KEY="AIza..."
```
--- 
## Usage
1. Configure the Evaluation: Open src/main.py to configure the test run. You can specify:
- Which LLM to test (e.g., OpenAIClient, GeminiClient).
- Which AF types to generate (af_generators_to_test).
- Which sizes (n) to test for each AF type (sizes_to_test).
2. Run the Evaluation: Execute the main.py script from the root directory to start the benchmarking process.
```bash
python src/main.py
```
3. View the Results: Upon completion, the results will be saved as a multi-sheet Excel file (e.g., gpt-4o.xlsx) in the reports/ directory. Each sheet in the file corresponds to one of the tested AF types.
